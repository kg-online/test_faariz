{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm85Ap3zDmYB"
      },
      "source": [
        "# Getting Started with MLC-LLM using the Llama 2 Model\n",
        "\n",
        "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ttPt-hNDmYC"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Let's set up your environment, so you can successfully run the `ChatModule`. First, let's set up the Conda environment which we will be running this notebook in (not required if running in Google Colab).\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```\n",
        "\n",
        "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
        "\n",
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KK25HZsIDmYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768b898a-3063-40d4-c08f-fcc5b787108a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  2 19:19:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOtpjJMDmYE"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PgW-5OAADmYE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34ef56be-c96b-4bfc-cc93-87c72779892c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1774-cp310-cp310-manylinux_2_28_x86_64.whl (511.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.4/511.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev557-cp310-cp310-manylinux_2_28_x86_64.whl (48.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Downloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.6/283.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna>=2.8 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.10.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.2\n",
            "    Uninstalling tornado-6.3.2:\n",
            "      Successfully uninstalled tornado-6.3.2\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.3\n",
            "    Uninstalling exceptiongroup-1.1.3:\n",
            "      Successfully uninstalled exceptiongroup-1.1.3\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.3\n",
            "    Uninstalling scipy-1.11.3:\n",
            "      Successfully uninstalled scipy-1.11.3\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\n",
            "tensorflow 2.14.0 requires ml-dtypes==0.2.0, but you have ml-dtypes 0.3.1 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-3.7.1 attrs-23.1.0 click-8.1.7 cloudpickle-3.0.0 decorator-5.1.1 exceptiongroup-1.1.3 fastapi-0.104.1 h11-0.14.0 idna-3.4 ml-dtypes-0.3.1 mlc-ai-nightly-cu118-0.12.dev1774 mlc-chat-nightly-cu118-0.1.dev557 numpy-1.26.1 psutil-5.9.6 pydantic-2.4.2 pydantic-core-2.10.1 scipy-1.11.3 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.3 typing-extensions-4.8.0 uvicorn-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn7MYEFt5tvY"
      },
      "source": [
        "**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwsWd1WbDmYE"
      },
      "source": [
        "Next, let's download the model weights for the Llama2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvAhErV3gjq"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0GjINnMDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbd2f74-6ece-4f21-f080-237b8fefa04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYwjsCOK7Jij"
      },
      "source": [
        "These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FSAe7Ew_DmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a41474-7359-4354-ed8b-396a6458b215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dist/prebuilt/lib'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 328 (delta 56), reused 54 (delta 54), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (328/328), 118.28 MiB | 26.18 MiB/s, done.\n",
            "Resolving deltas: 100% (237/237), done.\n",
            "Updating files: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BDbi6H3MDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee8ba10-758f-4b31-827f-9131ca3d13b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 129 (delta 0), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (129/129), 500.53 KiB | 3.65 MiB/s, done.\n",
            "Filtering content: 100% (116/116), 3.53 GiB | 65.23 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ru5__tDmYF"
      },
      "source": [
        "## Let's Chat!\n",
        "\n",
        "Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AJAt6oW7DmYF"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout\n",
        "\n",
        "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9m5sxyXDmYF"
      },
      "source": [
        "Note that the above invocation abstracts away the logic for finding the relevant model directory and prebuilt library paths. To specify these manually, you could run the following instead (which would be equivalent to the above).\n",
        "\n",
        "```python\n",
        "cm = ChatModule(model=\"dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\", lib_path=\"dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEaVXnnJDmYF"
      },
      "source": [
        "That is all what needed to set up the `ChatModule`. You can now chat with the model by entering any prompt you'd like. Try it out below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TNmg9N_NDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6edd2e20-724e-4361-d15b-0e587ac9d916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Agent Behaviour Metrics for this conversation are:\n",
            "1. Empathy: The agent shows empathy by acknowledging the customer's issue and expressing willingness to help.\n",
            "2. Active Listening: The agent actively listens to the customer's issue by asking clarifying questions and restating the issue back to the customer to ensure understanding.\n",
            "3. Problem-solving: The agent offers solutions to the customer's issue by suggesting ways to check the computer's resource usage and manage startup programs.\n",
            "4. Positive tone: The agent maintains a positive tone throughout the conversation, using phrases like \"I'm glad to hear that your computer is running smoother\" and \"We're here to help.\"\n",
            "5. Follow-up: The agent follows up with the customer to check if the issue has been resolved and offers additional assistance if needed.\n"
          ]
        }
      ],
      "source": [
        "# Summarize the following lengthy conversation to 300 words paragraph\n",
        "# Extract the keywords from the following lengthy conversation\n",
        "# Give a one line title for the following lengthy conversation\n",
        "# Identify the Agent Behaviour Metrics from the following lengthy conversation\n",
        "\n",
        "output = cm.generate(\n",
        "    prompt='''Identify the Agent Behaviour Metrics from the following lengthy conversation: Agent: Hello! Thank you for reaching out to our support team. How can I assist you today?\n",
        "Customer: Hi there! I'm having trouble with my computer. It's been acting really slow lately, and I'm not sure what's causing it.\n",
        "Agent: I'm sorry to hear that your computer is giving you trouble. Let's try to diagnose the issue. Can you please tell me a bit more about your computer? What's the make and model, and which operating system are you using?\n",
        "Customer: My computer is a Dell Inspiron 15, and I'm using Windows 10. I've had it for a couple of years now, and it used to work perfectly fine, but lately, it's become so slow that even simple tasks take forever to complete.\n",
        "Agent: Thank you for providing that information. Slowness issues can be caused by various factors. Let's start by checking a few things. Have you noticed any specific patterns of slowness, such as when opening specific applications or performing certain tasks?\n",
        "Customer: It seems to slow down when I'm running multiple applications simultaneously, like browsing the web while working in Microsoft Office. Sometimes, even just starting up the computer takes a long time.\n",
        "Agent: I see. Running multiple applications can indeed put a strain on your computer's resources. First, let's check your computer's resource usage. Press Ctrl+Shift+Esc to open the Task Manager. In the Task Manager, you'll see a list of running processes and their resource usage. Are there any specific processes that are consuming a lot of CPU or memory?\n",
        "Customer: I've opened the Task Manager, and it looks like the \"Antivirus Service\" and \"Windows Update\" are consuming a significant amount of CPU and memory. Is that normal?\n",
        "Agent: It's not unusual for your antivirus software and Windows Update to use some system resources, especially during updates or scans. However, if they are using an excessive amount of resources consistently, it might contribute to slowness. Have you noticed if Windows Update is stuck or if it's frequently running updates in the background?\n",
        "Customer: I do recall seeing some Windows updates recently. It's possible that they might be running in the background without my knowledge. What should I do in this case?\n",
        "Agent: If Windows Update is running updates, it can slow down your computer, especially if it's performing major updates. To check and potentially control Windows Update, follow these steps: Go to \"Settings\" > \"Update & Security\" > \"Windows Update.\" From there, you can view and control updates.\n",
        "Customer: Okay, I'm in the Windows Update settings now. It says there are some pending updates. What should I do next?\n",
        "Agent: You can choose to pause updates temporarily if you suspect they are causing the slowness. Click on \"Pause updates for 7 days\" to give your computer some breathing room. Once you've paused the updates, monitor your computer's performance and see if it improves.\n",
        "Customer: I've paused the updates, and my computer does seem to be running a bit faster. What should I do next?\n",
        "Agent: Great! While your updates are paused, let's also check your antivirus software. Some antivirus programs can be resource-intensive. You may want to open your antivirus software and adjust its settings to perform scans or updates at times when you're not actively using your computer.\n",
        "Customer: I'm using McAfee antivirus. I'll look into its settings and see if I can schedule scans and updates during off-peak hours. Is there anything else I should check?\n",
        "Agent: That sounds like a good plan. Additionally, you should ensure that your computer is free from unnecessary startup programs. Unnecessary startup programs can slow down your computer's boot time. You can manage startup programs in the Task Manager's \"Startup\" tab.\n",
        "Customer: I'll definitely check the startup programs. Thanks for your assistance so far! My computer already feels a bit faster. If I have any more questions or issues, can I reach out to you?\n",
        "Agent: You're very welcome! I'm glad to hear that your computer is running smoother. Absolutely, feel free to reach out if you have any more questions or encounter any other issues in the future. We're here to help. Have a great day!\n",
        "Customer: Thanks, you too! Have a wonderful day!\n",
        "Agent: Hello! Thank you for reaching out to our support team. How can I assist you today?\n",
        "Customer: Hi there! I'm having trouble with my computer. It's been acting really slow lately, and I'm not sure what's causing it.\n",
        "Agent: I'm sorry to hear that your computer is giving you trouble. Let's try to diagnose the issue. Can you please tell me a bit more about your computer? What's the make and model, and which operating system are you using?\n",
        "Customer: My computer is a Dell Inspiron 15, and I'm using Windows 10. I've had it for a couple of years now, and it used to work perfectly fine, but lately, it's become so slow that even simple tasks take forever to complete.\n",
        "Agent: Thank you for providing that information. Slowness issues can be caused by various factors. Let's start by checking a few things. Have you noticed any specific patterns of slowness, such as when opening specific applications or performing certain tasks?\n",
        "Customer: It seems to slow down when I'm running multiple applications simultaneously, like browsing the web while working in Microsoft Office. Sometimes, even just starting up the computer takes a long time.\n",
        "Agent: I see. Running multiple applications can indeed put a strain on your computer's resources. First, let's check your computer's resource usage. Press Ctrl+Shift+Esc to open the Task Manager. In the Task Manager, you'll see a list of running processes and their resource usage. Are there any specific processes that are consuming a lot of CPU or memory?\n",
        "Customer: I've opened the Task Manager, and it looks like the \"Antivirus Service\" and \"Windows Update\" are consuming a significant amount of CPU and memory. Is that normal?\n",
        "Agent: It's not unusual for your antivirus software and Windows Update to use some system resources, especially during updates or scans. However, if they are using an excessive amount of resources consistently, it might contribute to slowness. Have you noticed if Windows Update is stuck or if it's frequently running updates in the background?\n",
        "Customer: I do recall seeing some Windows updates recently. It's possible that they might be running in the background without my knowledge. What should I do in this case?\n",
        "Agent: If Windows Update is running updates, it can slow down your computer, especially if it's performing major updates. To check and potentially control Windows Update, follow these steps: Go to \"Settings\" > \"Update & Security\" > \"Windows Update.\" From there, you can view and control updates.\n",
        "Customer: Okay, I'm in the Windows Update settings now. It says there are some pending updates. What should I do next?\n",
        "Agent: You can choose to pause updates temporarily if you suspect they are causing the slowness. Click on \"Pause updates for 7 days\" to give your computer some breathing room. Once you've paused the updates, monitor your computer's performance and see if it improves.\n",
        "Customer: I've paused the updates, and my computer does seem to be running a bit faster. What should I do next?\n",
        "Agent: Great! While your updates are paused, let's also check your antivirus software. Some antivirus programs can be resource-intensive. You may want to open your antivirus software and adjust its settings to perform scans or updates at times when you're not actively using your computer.\n",
        "Customer: I'm using McAfee antivirus. I'll look into its settings and see if I can schedule scans and updates during off-peak hours. Is there anything else I should check?\n",
        "Agent: That sounds like a good plan. Additionally, you should ensure that your computer is free from unnecessary startup programs. Unnecessary startup programs can slow down your computer's boot time. You can manage startup programs in the Task Manager's \"Startup\" tab.\n",
        "Customer: I'll definitely check the startup programs. Thanks for your assistance so far! My computer already feels a bit faster. If I have any more questions or issues, can I reach out to you?\n",
        "Agent: You're very welcome! I'm glad to hear that your computer is running smoother. Absolutely, feel free to reach out if you have any more questions or encounter any other issues in the future. We're here to help. Have a great day!\n",
        "Customer: Thanks, you too! Have a wonderful day!\n",
        "''',\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_x5Gp_ZbUD"
      },
      "source": [
        "You can also repeat running the code block below for multiple rounds to interact with the model in a chat style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxcceNcZbUD",
        "outputId": "81afdcdf-e3f7-49f6-adb0-e1edba874dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is LLM\n",
            "LLM stands for Master of Laws, which is a postgraduate degree in law. It is a one-year or two-year program that is designed for students who have completed their undergraduate degree in law or a related field and want to specialize in a particular area of law.\n",
            "The LLM program typically involves coursework and sometimes a research paper or thesis. It is designed to provide students with advanced knowledge and skills in their chosen area of law, such as corporate law, intellectual property law, international law, or tax law.\n",
            "Earning an LLM degree can be beneficial for several reasons:\n",
            "1. Specialization: An LLM degree allows students to specialize in a particular area of law, which can be helpful for those who want to focus their career in a specific area.\n",
            "2. Career Advancement: An LLM degree can help lawyers advance their careers by providing them with advanced knowledge and skills that can make them more competitive in the job market.\n",
            "3. Networking: LLM programs provide opportunities to network with other lawyers and professionals in the field, which can be helpful for building a professional network.\n",
            "4. Academic Pursuits: An LLM degree can also be a stepping stone for those who want to pursue a career in academia or research.\n",
            "5. Personal Interest: Some lawyers may choose to pursue an LLM degree for personal interest or to explore a new area of law that they are interested in.\n",
            "It's worth noting that an LLM degree is not a requirement for practicing law, and it is not a substitute for a JD degree. It is a supplemental degree that provides additional knowledge and skills in a specific area of law.\n"
          ]
        }
      ],
      "source": [
        "prompt = input(\"Prompt: \")\n",
        "output = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vozceh9sZbUE",
        "outputId": "8b96b56e-a217-446d-c26e-d7a52a1321f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm here to help you with any questions you may have. However, I cannot provide personal information about specific individuals, including their names or identities. It's important to respect people's privacy and security by not sharing their personal details without their consent.\n",
            "If you're looking for information on a particular topic or subject, feel free to ask and I'll do my best to help!\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"Who is Sam?\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bOyUk7DmYF"
      },
      "source": [
        "To check the generation speed of the chat bot, you can print the statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PPbPj6vpDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91c6397-e906-4326-ae2e-b2c56aa9fbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefill: 32.4 tok/s, decode: 37.8 tok/s\n"
          ]
        }
      ],
      "source": [
        "print(cm.stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAb-XZPnDmYF"
      },
      "source": [
        "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iKpKgVxNDmYF"
      },
      "outputs": [],
      "source": [
        "cm.reset_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0eqwbsrZbUF"
      },
      "source": [
        "### Benchmark Performance\n",
        "\n",
        "To benchmark the performance, we can use the `benchmark_generate` method of ChatModule. It takes an input prompt and the number of tokens to generate, ignores the system prompt and model stop criterion, generates tokens in a language model way and stops until finishing generating the desired number of tokens. After calling `benchmark_generate`, we can use `stats` to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fmPAkv-sZbUF",
        "outputId": "a83c0f60-8e55-4db3-dc0f-0d58596e4bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on the stock market. Unterscheidung zwischen \"Investment\" und \"Spekulation\". While the term \"investment\" is often used interchangeably with \"speculation,\" they have distinct meanings. Investment refers to the act of putting money into something with the expectation of earning a profit in the future, often through regular interest payments or dividends. Speculation, on the other hand, refers to the act of buying or selling an asset with the hope of making a quick profit, often through fluctuations in market prices.\n",
            "\n",
            "{ \"@type\": \"Question\", \"name\": \"What is the difference between investment and speculation?\", \"acceptedAnswer\": { \"@type\": \"Answer\", \"text\": \"Investment refers to the act of putting money into something with the expectation of earning a profit in the future, often through regular interest payments or dividends. Speculation, on the other hand, refers to the act of buying or selling an asset with the hope of making a quick profit, often through fluctuations in market prices. While both activities involve using money to make a profit, the key difference is that investment involves a long-term perspective and a focus on generating steady income, while speculation involves a shorter-term focus and a higher level of risk. \" } }\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prefill: 28.1 tok/s, decode: 36.8 tok/s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "print(cm.benchmark_generate(prompt=\"I lost money\", generate_length=512))\n",
        "cm.stats()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjPq1KdVf7Pv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}